import requests
import re
from collections import defaultdict
import time

# -------------------------
# é¢‘é“åˆ†ç±»ï¼ˆæ­£è§„åŒºåŸŸï¼‰
# -------------------------
CHANNEL_CATEGORIES = {
    "å¤®è§†é¢‘é“": ['CCTV1', 'CCTV2', 'CCTV3', 'CCTV4', 'CCTV4æ¬§æ´²', 'CCTV4ç¾æ´²', 'CCTV5', 'CCTV5+', 'CCTV6', 'CCTV7', 'CCTV8', 'CCTV9',
                 'CCTV10', 'CCTV11', 'CCTV12', 'CCTV13', 'CCTV14', 'CCTV15', 'CCTV16', 'CCTV17', 'å…µå™¨ç§‘æŠ€', 'é£äº‘éŸ³ä¹', 'é£äº‘è¶³çƒ',
                 'é£äº‘å‰§åœº', 'æ€€æ—§å‰§åœº', 'ç¬¬ä¸€å‰§åœº', 'å¥³æ€§æ—¶å°š', 'ä¸–ç•Œåœ°ç†', 'å¤®è§†å°çƒ', 'é«˜å°”å¤«ç½‘çƒ', 'å¤®è§†æ–‡åŒ–ç²¾å“', 'å«ç”Ÿå¥åº·', 'ç”µè§†æŒ‡å—'],
    "å«è§†é¢‘é“": ['æ¹–å—å«è§†', 'æµ™æ±Ÿå«è§†', 'æ±Ÿè‹å«è§†', 'ä¸œæ–¹å«è§†', 'æ·±åœ³å«è§†', 'åŒ—äº¬å«è§†', 'å¹¿ä¸œå«è§†', 'å¹¿è¥¿å«è§†', 'ä¸œå—å«è§†', 'æµ·å—å«è§†',
                 'æ²³åŒ—å«è§†', 'æ²³å—å«è§†', 'æ¹–åŒ—å«è§†', 'æ±Ÿè¥¿å«è§†', 'å››å·å«è§†', 'é‡åº†å«è§†', 'è´µå·å«è§†', 'äº‘å—å«è§†', 'å¤©æ´¥å«è§†', 'å®‰å¾½å«è§†',
                 'å±±ä¸œå«è§†', 'è¾½å®å«è§†', 'é»‘é¾™æ±Ÿå«è§†', 'å‰æ—å«è§†', 'å†…è’™å¤å«è§†', 'å®å¤å«è§†', 'å±±è¥¿å«è§†', 'é™•è¥¿å«è§†', 'ç”˜è‚ƒå«è§†',
                 'é’æµ·å«è§†', 'æ–°ç–†å«è§†', 'è¥¿è—å«è§†', 'ä¸‰æ²™å«è§†', 'å¦é—¨å«è§†', 'å…µå›¢å«è§†', 'å»¶è¾¹å«è§†', 'å®‰å¤šå«è§†', 'åº·å·´å«è§†', 'å†œæ—å«è§†', 'å±±ä¸œæ•™è‚²',
                 'CETV1', 'CETV2', 'CETV3', 'CETV4', 'æ—©æœŸæ•™è‚²'],
    "æ•°å­—é¢‘é“": ['CHCåŠ¨ä½œç”µå½±', 'CHCå®¶åº­å½±é™¢', 'CHCå½±è¿·ç”µå½±', 'æ·˜ç”µå½±', 'æ·˜ç²¾å½©', 'æ·˜å‰§åœº', 'æ·˜4K', 'æ·˜å¨±ä¹', 'æ·˜Baby', 'èŒå® TV', 'åŒ—äº¬çºªå®ç§‘æ•™', 'é‡æ¸©ç»å…¸',
                 'æ˜Ÿç©ºå«è§†', 'CHANNEL[V]', 'å‡¤å‡°ä¸­æ–‡', 'å‡¤å‡°èµ„è®¯', 'å‡¤å‡°é¦™æ¸¯', 'å‡¤å‡°ç”µå½±', 'æ±‚ç´¢çºªå½•', 'æ±‚ç´¢ç§‘å­¦', 'æ±‚ç´¢ç”Ÿæ´»', 'æ±‚ç´¢åŠ¨ç‰©',
                 'ç›å½©é’å°‘', 'ç›å½©ç«æŠ€', 'ç›å½©ç¯®çƒ', 'ç›å½©å¹¿åœºèˆ', 'é‡‘é¹°çºªå®', 'å¿«ä¹å‚é’“', 'èŒ¶é¢‘é“', 'å¤©å…ƒå›´æ£‹', 'é­…åŠ›è¶³çƒ', 'äº”æ˜Ÿä½“è‚²', 'åŠ²çˆ†ä½“è‚²',
                 'ä¹æ¸¸', 'ç”Ÿæ´»æ—¶å°š', 'éƒ½å¸‚å‰§åœº', 'æ¬¢ç¬‘å‰§åœº', 'æ¸¸æˆé£äº‘', 'åŠ¨æ¼«ç§€åœº', 'é‡‘è‰²å­¦å ‚', 'æ³•æ²»å¤©åœ°', 'å“’å•µèµ›äº‹', 'å“’å•µç”µç«', 'é»‘è“ç”µå½±', 'é»‘è“åŠ¨ç”»', 
                 'å¡é…·å°‘å„¿', 'é‡‘é¹°å¡é€š', 'ä¼˜æ¼«å¡é€š', 'å“ˆå“ˆç‚«åŠ¨', 'å˜‰ä½³å¡é€š', 'iHOTçˆ±å–œå‰§', 'iHOTçˆ±ç§‘å¹»', 'iHOTçˆ±é™¢çº¿', 'iHOTçˆ±æ‚¬ç–‘',
                 'iHOTçˆ±å†å²', 'iHOTçˆ±è°æˆ˜', 'iHOTçˆ±æ—…è¡Œ', 'iHOTçˆ±å¹¼æ•™', 'iHOTçˆ±ç©å…·', 'iHOTçˆ±ä½“è‚²', 'iHOTçˆ±èµ›è½¦', 'iHOTçˆ±æµªæ¼«', 'iHOTçˆ±å¥‡è°ˆ',
                 'iHOTçˆ±ç§‘å­¦', 'iHOTçˆ±åŠ¨æ¼«', 'ä¸œåŒ—çƒ­å‰§', 'ä¸­å›½åŠŸå¤«', 'åŠ¨ä½œç”µå½±', 'å†›äº‹è¯„è®º', 'å†›æ—…å‰§åœº', 'é­…åŠ›æ½‡æ¹˜',
                 'å¤è£…å‰§åœº', 'å®¶åº­å‰§åœº', 'æƒŠæ‚šæ‚¬ç–‘', 'æ˜æ˜Ÿå¤§ç‰‡', 'æ¬¢ä¹å‰§åœº', 'æµ·å¤–å‰§åœº', 'æ½®å¦ˆè¾£å©†', 'çˆ±æƒ…å–œå‰§',
                 'ç‚«èˆæœªæ¥', 'ç²¾å“ä½“è‚²', 'ç²¾å“å¤§å‰§', 'ç²¾å“çºªå½•', 'ç²¾å“èŒå® ', 'è¶…çº§ä½“è‚²', 'è¶…çº§ç”µå½±', 'æ€¡ä¼´å¥åº·',
                 'è¶…çº§ç”µè§†å‰§', 'è¶…çº§ç»¼è‰º', 'é‡‘ç‰Œç»¼è‰º', 'æ­¦æä¸–ç•Œ', 'å†œä¸šè‡´å¯Œ'],
    "å±±è¥¿é¢‘é“": ['å±±è¥¿å«è§†', 'å±±è¥¿é»„æ²³HD', 'å±±è¥¿ç»æµä¸ç§‘æŠ€HD', 'å±±è¥¿å½±è§†HD', 'å±±è¥¿ç¤¾ä¼šä¸æ³•æ²»HD', 'å±±è¥¿æ–‡ä½“ç”Ÿæ´»HD'],
}

# -------------------------
# é¢‘é“æ˜ å°„ï¼ˆåˆ«å -> è§„èŒƒåï¼‰
# -------------------------
CHANNEL_MAPPING = {
    "CCTV1": ["CCTV-1", "CCTV-1 HD", "CCTV-1 ç»¼åˆ", "CCTV1 HD", "CCTV 1", "CCTV-1 é«˜æ¸…"],
    "CCTV2": ["CCTV-2", "CCTV-2 HD", "CCTV-2 è´¢ç»", "CCTV2 HD", "CCTV 2", "CCTV-2 é«˜æ¸…"],
    "CCTV3": ["CCTV-3", "CCTV-3 HD", "CCTV-3 ç»¼è‰º", "CCTV3 HD", "CCTV 3", "CCTV-3 é«˜æ¸…"],
    "CCTV4": ["CCTV-4", "CCTV-4 HD", "CCTV4a", "CCTV4A", "CCTV-4 ä¸­æ–‡å›½é™…", "CCTV4 é«˜æ¸…"],
    "CCTV4æ¬§æ´²": ["CCTV-4æ¬§æ´²", "CCTV-4æ¬§æ´² HD", "CCTV-4 æ¬§æ´²", "CCTV4o", "CCTV4O", "CCTV-4 ä¸­æ–‡æ¬§æ´²", "CCTV4ä¸­æ–‡æ¬§æ´²"],
    "CCTV4ç¾æ´²": ["CCTV-4ç¾æ´²", "CCTV-4ç¾æ´² HD", "CCTV-4 ç¾æ´²", "CCTV4m", "CCTV4M", "CCTV-4 ä¸­æ–‡ç¾æ´²", "CCTV4ä¸­æ–‡ç¾æ´²"],
    "CCTV5": ["CCTV-5", "CCTV-5 HD", "CCTV-5 ä½“è‚²", "CCTV5 HD", "CCTV 5", "CCTV-5 é«˜æ¸…"],
    "CCTV5+": ["CCTV-5+", "CCTV-5+ HD", "CCTV-5+ ä½“è‚²èµ›äº‹", "CCTV5+ HD", "CCTV 5+"],
    "CCTV6": ["CCTV-6", "CCTV-6 HD", "CCTV-6 ç”µå½±", "CCTV6 HD", "CCTV 6"],
    "CCTV7": ["CCTV-7", "CCTV-7 HD", "CCTV-7 å›½é˜²å†›äº‹", "CCTV7 HD", "CCTV 7"],
    "CCTV8": ["CCTV-8", "CCTV-8 HD", "CCTV-8 ç”µè§†å‰§", "CCTV8 HD", "CCTV 8"],
    "CCTV9": ["CCTV-9", "CCTV-9 HD", "CCTV-9 çºªå½•", "CCTV9 HD", "CCTV 9"],
    "CCTV10": ["CCTV-10", "CCTV-10 HD", "CCTV-10 ç§‘æ•™", "CCTV10 HD", "CCTV 10"],
    "CCTV11": ["CCTV-11", "CCTV-11 HD", "CCTV-11 æˆæ›²", "CCTV11 HD", "CCTV 11"],
    "CCTV12": ["CCTV-12", "CCTV-12 HD", "CCTV-12 ç¤¾ä¼šä¸æ³•", "CCTV12 HD", "CCTV 12"],
    "CCTV13": ["CCTV-13", "CCTV-13 HD", "CCTV-13 æ–°é—»", "CCTV13 HD", "CCTV 13"],
    "CCTV14": ["CCTV-14", "CCTV-14 HD", "CCTV-14 å°‘å„¿", "CCTV14 HD", "CCTV 14"],
    "CCTV15": ["CCTV-15", "CCTV-15 HD", "CCTV-15 éŸ³ä¹", "CCTV15 HD", "CCTV 15"],
    "CCTV16": ["CCTV-16", "CCTV-16 HD", "CCTV-16 å¥¥æ—åŒ¹å…‹", "CCTV16 4K", "CCTV16å¥¥æ—åŒ¹å…‹ 4K"],
    "CCTV17": ["CCTV-17", "CCTV-17 HD", "CCTV-17 å†œä¸šå†œæ‘", "CCTV17 HD", "CCTV 17"],
    "æ¹–å—å«è§†": ["æ¹–å—å«è§†é«˜æ¸…", "æ¹–å—å«è§† HD", "æ¹–å—å«è§†HD", "æ¹–å—å«è§† é«˜æ¸…"],
    "æµ™æ±Ÿå«è§†": ["æµ™æ±Ÿå«è§†é«˜æ¸…", "æµ™æ±Ÿå«è§† HD", "æµ™æ±Ÿå«è§†HD", "æµ™æ±Ÿå«è§† é«˜æ¸…"],
    "æ±Ÿè‹å«è§†": ["æ±Ÿè‹å«è§†é«˜æ¸…", "æ±Ÿè‹å«è§† HD", "æ±Ÿè‹å«è§†HD", "æ±Ÿè‹å«è§† é«˜æ¸…"],
    "ä¸œæ–¹å«è§†": ["ä¸œæ–¹å«è§†é«˜æ¸…", "ä¸œæ–¹å«è§† HD", "ä¸œæ–¹å«è§†HD", "ä¸œæ–¹å«è§† é«˜æ¸…"],
    "åŒ—äº¬å«è§†": ["åŒ—äº¬å«è§†é«˜æ¸…", "åŒ—äº¬å«è§† HD", "åŒ—äº¬å«è§†HD", "åŒ—äº¬å«è§† é«˜æ¸…"],
    # å…¶ä»–æ˜ å°„ä¿æŒä¸å˜...
}

# å…¶ä»–æ˜ å°„ä¿æŒä¸å˜...
# ä¸ºäº†ä¿æŒä»£ç ç®€æ´ï¼Œè¿™é‡Œçœç•¥äº†å®Œæ•´çš„æ˜ å°„è¡¨ï¼Œä½ å¯ä»¥ä¿ç•™åŸæœ‰çš„CHANNEL_MAPPING

# -------------------------
# æ­£åˆ™è¡¨è¾¾å¼
# -------------------------
ipv6_regex = r"https?://\[[0-9a-fA-F:]+\]"
ipv4_regex = r"https?://[^\s]+"

def normalize_channel_name(name: str) -> str:
    """æ ¹æ®åˆ«åæ˜ å°„è¡¨ç»Ÿä¸€é¢‘é“åç§°"""
    name = re.sub(r'\s+', ' ', name.strip())  # å»é™¤å¤šä½™ç©ºæ ¼
    for standard, aliases in CHANNEL_MAPPING.items():
        if name == standard:
            return standard
        for alias in aliases:
            if alias.lower() == name.lower():
                return standard
    return name

def is_invalid_url(url: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºæ— æ•ˆ URL"""
    invalid_patterns = [
        r"http://\[[a-fA-F0-9:]+\](?::\d+)?/ottrrs\.hl\.chinamobile\.com/.+/.+",
        r"http://\[2409:8087:1a01:df::7005\]/.*",
        r".*\.m3u8?$",  # æ’é™¤m3u8é“¾æ¥ï¼ˆå¯é€‰ï¼‰
    ]
    
    for pattern in invalid_patterns:
        if re.match(pattern, url):
            return True
    return False

# -------------------------
# æŠ“å– URL
# -------------------------
def fetch_lines(url: str, max_retries=3):
    """ä¸‹è½½å¹¶åˆ†è¡Œè¿”å›å†…å®¹ï¼Œæ”¯æŒé‡è¯•"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for retry in range(max_retries):
        try:
            resp = requests.get(url, headers=headers, timeout=15)
            resp.encoding = resp.apparent_encoding or 'utf-8'
            if resp.status_code == 200:
                return resp.text.splitlines()
            else:
                print(f"âš ï¸  HTTP {resp.status_code} ä» {url}")
        except Exception as e:
            print(f"âŒ è·å–å¤±è´¥ {url} (å°è¯• {retry+1}/{max_retries}): {e}")
            if retry < max_retries - 1:
                time.sleep(2)  # é‡è¯•å‰ç­‰å¾…
    return []

# -------------------------
# è§£æ M3U / TXT
# -------------------------
def parse_lines(lines):
    """è§£æ M3U æˆ– TXT å†…å®¹ï¼Œè¿”å› {é¢‘é“å: [urlåˆ—è¡¨]}"""
    channels_dict = defaultdict(list)
    current_name = None
    group_title = None
    
    for i, line in enumerate(lines):
        line = line.strip()
        if not line:
            continue

        # M3U #EXTINF æ ¼å¼
        if line.startswith("#EXTINF"):
            current_name = None
            group_title = None
            
            # æå–é¢‘é“å
            if "," in line:
                current_name = line.split(",")[-1].strip()
            
            # å°è¯•ä»å±æ€§ä¸­æå– group-title
            match = re.search(r'group-title="([^"]+)"', line)
            if match:
                group_title = match.group(1)
            
            # å°è¯•æå– tvg-name
            match = re.search(r'tvg-name="([^"]+)"', line)
            if match and not current_name:
                current_name = match.group(1)
        
        # å¦‚æœæ˜¯URLè¡Œä¸”æˆ‘ä»¬æœ‰é¢‘é“å
        elif current_name and (line.startswith("http://") or line.startswith("https://")):
            url = line.split("$")[0].strip()  # å»æ‰ $ åç¼€
            
            # æ£€æŸ¥URLæœ‰æ•ˆæ€§
            if not is_invalid_url(url):
                # ä¼˜å…ˆä½¿ç”¨tvg-nameï¼Œå¦åˆ™ä½¿ç”¨é¢‘é“å
                norm_name = normalize_channel_name(current_name)
                if norm_name:
                    channels_dict[norm_name].append({
                        'url': url,
                        'group': group_title
                    })
            current_name = None
            group_title = None
        
        # TXT é¢‘é“å,URL æ ¼å¼
        elif "," in line and not line.startswith("#"):
            parts = line.split(",", 1)
            if len(parts) == 2:
                ch_name, url = parts[0].strip(), parts[1].strip()
                url = url.split("$")[0].strip()
                if not is_invalid_url(url):
                    norm_name = normalize_channel_name(ch_name)
                    if norm_name:
                        channels_dict[norm_name].append({
                            'url': url,
                            'group': None
                        })

    return channels_dict

# -------------------------
# å»é‡å’Œæ’åºURL
# -------------------------
def deduplicate_urls(url_list):
    """å»é‡URLï¼Œä¿ç•™é¡ºåº"""
    seen = set()
    unique_urls = []
    for item in url_list:
        url = item['url']
        if url not in seen:
            seen.add(url)
            unique_urls.append(item)
    return unique_urls

# -------------------------
# ç”Ÿæˆ M3U æ–‡ä»¶
# -------------------------
def create_m3u_file(all_channels, filename="ipv6.m3u"):
    """ç”Ÿæˆå¸¦åˆ†ç±»çš„ M3U æ–‡ä»¶ï¼Œä¸€é¢‘é“å¤šæºè¿ç»­å†™"""
    channel_count = 0
    url_count = 0
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write('#EXTM3U x-tvg-url="https://kakaxi-1.github.io/IPTV/epg.xml"\n\n')
        
        for group, channel_list in CHANNEL_CATEGORIES.items():
            for ch in channel_list:
                if ch in all_channels and all_channels[ch]:
                    # å»é‡ URLï¼Œä¿ç•™é¡ºåº
                    unique_urls = deduplicate_urls(all_channels[ch])
                    
                    if unique_urls:
                        logo = f"https://kakaxi-1.github.io/IPTV/LOGO/{ch}.png"
                        f.write(f'#EXTINF:-1 tvg-id="{ch}" tvg-name="{ch}" tvg-logo="{logo}" group-title="{group}",{ch}\n')
                        for item in unique_urls:
                            f.write(f"{item['url']}\n")
                        f.write("\n")
                        
                        channel_count += 1
                        url_count += len(unique_urls)
    
    return channel_count, url_count

# -------------------------
# æ·»åŠ å¤‡ç”¨æº
# -------------------------
def get_backup_sources():
    """è¿”å›å¤‡ç”¨æºåˆ—è¡¨"""
    return [
        "https://raw.githubusercontent.com/kakaxi-1/IPTV/main/ipv6.m3u",
        "https://raw.githubusercontent.com/YanG-1989/m3u/main/China.m3u",
        "https://raw.githubusercontent.com/fanmingming/live/main/tv/m3u/ipv6.m3u",
        "https://raw.githubusercontent.com/guptaharsh1997/IPTV/main/playlist.m3u",
        # å¯ä»¥æ·»åŠ æ›´å¤šæº
    ]

# -------------------------
# ä¸»å‡½æ•°
# -------------------------
def main():
    print("ğŸ”„ å¼€å§‹æŠ“å–IPTVæº...")
    
    all_channels = defaultdict(list)
    sources = get_backup_sources()
    
    for idx, url in enumerate(sources, 1):
        print(f"\nğŸ“¡ æ­£åœ¨å¤„ç†æº {idx}/{len(sources)}: {url}")
        lines = fetch_lines(url)
        if lines:
            parsed = parse_lines(lines)
            found_count = len(parsed)
            for ch, urls_list in parsed.items():
                all_channels[ch].extend(urls_list)
            print(f"   âœ… æ‰¾åˆ° {found_count} ä¸ªé¢‘é“")
        else:
            print("   âš ï¸  æœªè·å–åˆ°æ•°æ®")
    
    # ç”Ÿæˆæ–‡ä»¶
    print("\nğŸ“ æ­£åœ¨ç”ŸæˆM3Uæ–‡ä»¶...")
    channel_count, url_count = create_m3u_file(all_channels, "ipv6_merged.m3u")
    
    print(f"\nâœ… å®Œæˆï¼")
    print(f"   é¢‘é“æ€»æ•°: {channel_count}")
    print(f"   æºæ€»æ•°: {url_count}")
    print(f"   æ–‡ä»¶å·²ä¿å­˜ä¸º: ipv6_merged.m3u")
    
    # æ˜¾ç¤ºéƒ¨åˆ†é¢‘é“ç»Ÿè®¡
    print(f"\nğŸ“Š é¢‘é“åˆ†ç±»ç»Ÿè®¡:")
    for group, channels in CHANNEL_CATEGORIES.items():
        count = sum(1 for ch in channels if ch in all_channels and all_channels[ch])
        if count > 0:
            print(f"   {group}: {count}/{len(channels)}")

if __name__ == "__main__":
    main()


